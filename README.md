# ğŸ§  Chooseâ€‘better

## ğŸ“– Overview  
**Chooseâ€‘better** is an intelligent Python project that merges **multiâ€‘website web scraping** with **dataâ€‘driven algorithmic analysis**.  
It automatically collects product or deal data from top eâ€‘commerce sites like **Amazon, Flipkart, ShopClues, Myntra, and Snapdeal**, and then processes that information using **optimization algorithms** (Greedy, Knapsack, Heap Sort, etc.) to help users *choose better* among competing options.

This project demonstrates how **real-world data scraping** and **classical algorithms** can work handâ€‘inâ€‘hand for analytical and decisionâ€‘support systems.

---

## ğŸš€ Key Features  

### ğŸŒ Web Scraping Suite  
The `scraping/` folder includes separate scrapers for each major e-commerce platform:  
- ğŸ›’ `amazon.py` â€” Extracts product names, prices, ratings, and links from Amazon.  
- ğŸ¬ `flipkart.py` â€” Gathers detailed product listings from Flipkart, handling pagination and dynamic content.  
- ğŸ›ï¸ `myntra.py` â€” Fetches fashion items and price data from Myntra.  
- ğŸ§¾ `shopclues.py` â€” Scrapes item names, discounts, and availability from ShopClues.  
- ğŸ’» `snapdeal.py` â€” Captures product details and ratings from Snapdeal.  

Each scraper uses **BeautifulSoup** and/or **Selenium** depending on whether the page is static or dynamically loaded.

### âš™ï¸ Selenium Utilities  
Located in the `scraping/`:  
- Automates browser sessions (Chrome WebDriver).  
- Waits for elements to load using `WebDriverWait`.  
- Includes smart delays and retry logic to avoid detection.  
- Manages dynamic content rendering and JavaScript-heavy pages.

### ğŸ§® Algorithms & Data Structures  
After scraping, collected data is processed using algorithms from the root modules:  
- **`datastructures.py`** â€” Defines reusable structures for managing product and price data.  
- **`greedy.py`** â€” Implements greedy decision logic for selecting the best options under constraints.  
- **`heap_sort.py`** â€” Sorts large datasets efficiently based on user-defined metrics (like price or rating).  
- **`knapsack.py`** â€” Solves the 0/1 Knapsack problem for optimal budget allocation (maximize value under price limit).  
- **`utilities.py`** â€” Common helpers for data cleaning, formatting, logging, and visualization.  

### ğŸ§© Integration (main.py)  
`main.py` connects all modules â€” scraping, cleaning, and analysis â€” to produce actionable results like:  
- Sorted product lists by best price-to-rating ratio.  
- Optimal product combination under a fixed budget.  
- Cross-platform comparisons of the same item.

---

## ğŸ—ï¸ Project Structure  
```
Choose-better/
â”‚
â”œâ”€â”€ scraping/
â”‚   â”œâ”€â”€ amazon.py
â”‚   â”œâ”€â”€ flipkart.py
â”‚   â”œâ”€â”€ shopclues.py
â”‚   â”œâ”€â”€ myntra.py
â”‚   â”œâ”€â”€ snapdeal.py
â”‚   â””â”€â”€ selenium_utils.py      # Selenium setup and browser automation helpers
â”‚
â”œâ”€â”€ datastructures.py
â”œâ”€â”€ greedy.py
â”œâ”€â”€ heap_sort.py
â”œâ”€â”€ knapsack.py
â”œâ”€â”€ utilities.py
â”œâ”€â”€ main.py
â””â”€â”€ __pycache__/
```

---

## âš™ï¸ Installation & Setup  

### 1ï¸âƒ£ Clone the repository  
```bash
git clone https://github.com/Varunesh07/Choose-better.git
cd Choose-better
```

### 2ï¸âƒ£ Install dependencies  
Make sure **Python 3.8+** is installed. Then run:  
```bash
pip install -r requirements.txt
```

**Typical dependencies include:**  
```
requests
beautifulsoup4
selenium
webdriver-manager
lxml
logging
math
```
*(If you donâ€™t have a requirements file, you can manually install them using pip.)*

### 3ï¸âƒ£ Configure Selenium (if using Chrome)  
Make sure Chrome browser and [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/) are installed.  
The `webdriver_manager` automatically handles driver setup.

### 4ï¸âƒ£ Run the Project  
To scrape and process all data:  
```bash
python main.py
```

To test individual modules:  
```bash
python scraping/amazon.py
python greedy.py
python knapsack.py
```

---

## ğŸ§  Workflow Summary  
1. **Data Collection:**  
   Each scraper collects product data (price, rating, etc.) from its respective site.  
2. **Data Cleaning & Storage:**  
   Data is cleaned, filtered, and optionally saved to a CSV/JSON file.  
3. **Algorithmic Analysis:**  
   The main script uses sorting, greedy optimization, or knapsack methods to find the best product(s).  
4. **Results Display:**  
   Outputs a ranked list or optimal set of products with maximum value under a given constraint.

---

## ğŸ§© Example Use Cases  
- ğŸ›ï¸ **Find Best Deals:** Compare same products across multiple platforms.  
- ğŸ’¸ **Optimize Spending:** Select items that maximize value under a budget limit (Knapsack).  
- ğŸ“ˆ **Rank Products:** Sort by weighted metrics using Heap Sort.  
- âš¡ **Quick Picks:** Use Greedy heuristics for fast, near-optimal results.

---

## Main Contributors

<table> <tr>  <td align="center"> <a href="https://github.com/abhx12"> <img src="https://avatars.githubusercontent.com/u/195339058??v=4" width="100px;" alt="Nithiish SD"/> <br /> <sub><b>Abhishek J R</b></sub> </a> </td> <td align="center"> <a href="https://github.com/Varunesh07"> <img src="https://avatars.githubusercontent.com/u/205139899?v=4" width="100px;" alt="Varunesh S"/> <br /> <sub><b>Varunesh S</b></sub> </a> </td> <td align="center"> <a href="https://github.com/Monarch0703"> <img src="https://avatars.githubusercontent.com/u/118116807?v=4" width="100px;" alt="Harshith Shiva"/> <br /> <sub><b>Jithendra U</b></sub> </a> </td> </tr> </table>

